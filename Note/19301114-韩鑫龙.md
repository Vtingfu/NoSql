# 论文阅读笔记

## 1. 前序

​		交大软件人这个学期过于繁忙, 老哥发来的论文没有做到字句精读, 但对其中的分量有所感知。本人目前打算硕士阶段主攻分布式系统软件工程，所以对于这样有价值的学习材料还是十分珍惜的，但奈何时间有限，此次阅读之作为了解，后续必然回过头来重新阅读。

## 2. 阅读笔记

​		对于如今的大数据来说，单机的性能已经不足以处理，意味着分布式计算有着不可替代的作用。这正这几篇论文所要讲的主要内容，以下是我在阅读过程中的一些感想

### MapReduce

​		MapReduce 是一个模型，是一个简单到我们只需要我们想要执行的运算即可，而那些并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在一个库中，我们能直接调用，接口包括Map 和 Reduce。

​		MapReduce 编程模型的原理：利用一个输入key/value pair集合来产生一个输出的key/value pair集合，例如在一个大的文档中计算每个单词出现的的次数，它首先用Map函数输出文档中的每个词、以及这个词的出现次数，Reduce函数把Map函数产生的每一个特定的词的计数累加起来。然后再用户的代码中使用一个可选的调节参数来完成一个符合MapReduce模型规范的对象，在使用中，直接调用MapReduce库，链接在一起，便可实现。

 		MapReduce模型可以有多种不同的实现方式: 有的适用于小型的共享内存方式的机器，有的则适用于大型NUMA架构的多处理器的主机，而有的实现方式更适合人型的网络连接集群。

 		对于分布式集群，Master持有一些数据结构,它储存每一个Map和Reduce任务的状态，以及Worker机器的标识。Maser就像一个数据管道，中向文件存储区域的位畳信息通过这个管道从Map传递到Reduce.因此,对于每个已完成的Map任务, master存儲了Map任务产生的文件存儲区域的大小和位置。当Map任务完成时，Maser 接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的Reduce任务。同时，MapReduce中还有着良好的容错机制。因为MapReduce库的设计初衷是使用由成百上千的机器组成的集群来处理超大规模的数据，所以，这个库必须要很好的能够处理机器故障。虽然简单的Map和Reduce函数提供的基本功能已经能够满足大部分的计算需要，但是还是发掘出了一些有价值的扩展功能。例如，分区函数、顺序保证、跳过损坏的记录、本地执行、状态信息、与计数器等方面。

​		关于MapReduce的性能，一个计算在大约1TB的数据中进行特定的模式匹配，另一个计算对大约1TB的数据进行排序。这两个程序在大量的使用MapReduce的实际应用中是非常典型的一一类是对数据格式进行转换，从一种表现形式转换为另外一种表现形式:另一类是从海量数据中抽取少部分的用户感兴趣的数据。

​		MapReduce编程模型在Google内部成功应用于多个领域。原因有这几个方面:首先，由于MapReduce封装了并行处理、容错处理、数据本地化优化、负载均衡等等技术难点的细节，这使得MapReduce库易于使用。即便对于完全没有并行或者分布式系统开发经验的程序员而言，其次，大量不同类型的问题都可以通过MapReduce简单的解决。比如，MapReduce用于生成Google的网络搜索服务所需要的数据、用来排序、用来数据挖掘、用于机器学习，以及很多其它的系统;第三，我们实现了一个在数千台计算机组成的大型集群，上灵活部署运行的MapReduce。这个实现使得有效利用这些丰富的计算资源变得非常简单，因此也适合用来解决Google遇到的其他很多需要大量计算的问题。

#### The Google File System

​		GFS是Google为其内部应用设计的分布式存储系统。Google可能是这个星球上最大的数据工厂了。如何高效可靠地存储如此大规模的数据成为一个很棘手的问题。纵观Google的内部应用，数据访问有以下特点：数据集庞大，数据总量和单个文件都比较大，如应用常常产生数GB大小的单个文件；数据访问特点多为顺序访问，比较常见的场景是数据分析，应用程序会顺序遍历数据文件，产生顺序读行为；多客户端并发追加场景很常见，极少有随机写行为；一次写入，多次读取，例如互联网上的网页存储。

​		GFS系统主要存储文件系统两类数据：文件元数据：包括file system namespace（目录结构、文件元数据等）、文件存储位置信息等；和文件数据：文件在磁盘上的存储格式

​		Master是存储系统元数据信息，主要包括namespace、文件chunk信息以及chunk多副本位置信息。Master是系统的中心节点，所有客户端的元数据访问，如列举目录下文件，获取文件属性等操作都是直接访问Master。除此之外，还承担了系统诸多的管理工作。

​		over-write由客户端指定文件更新offset。当客户端是串行更新时，客户端自己知道写入文件范围以及写入数据内容，且本次写入在数据服务器的多副本上均执行成功。因此，本次写结果对于客户端来说就是明确的，且多副本上数据一致，故而结果是defined。

​		并行写入时多个客户端由于写入范围可能交叉而形成交织写。这时候，由于单个客户端无法决定写入顺序（只有主副本才能决定谁先写谁后写），因此，即使写入成功，客户端仍无法确定在并发写入时交叉部分最终写入结果，但是因为写入成功，所以多副本数据必然一致。

​		无论是穿串行还是并行over-write，一旦失败，多个chunk副本上的数据可能都不一致了，其次，客户端从不同的副本上读出的数据也不一样（可能某些副本成功而某些副本失败），因此，必然也是undefined，也是inconsistent。

​		append操作无需指定offset，由chunk主副本根据当前文件大小决定写入offset，在写入成功后将该offset返回给客户端。因此，客户端能够根据offset确切知道写入结果，无论是串行写入还是并发写入，其行为是defined。GFS在chunk多副本之间选择出一个主副本，由主副本来协调客户端的写入，保证多副本之间维持一个全局统一的更新顺序，GFS使用了租约。

​		Lease是由GFS中心节点Master分配给chunk的某个副本的锁。持有租约的副本方可处理客户端的更新请求，客户端更新数据前会从Master获取该chunk持有租约的副本并向该副本发送更新请求。本质上是一种有时间限制的锁：租约的持有者（chunk的某个副本）需要定期向Master申请续约。如果超过租约的期限，那么该租约会被强制收回并重新分配给其他副本。