# 非关论文读后报告

> 19301024
>
> 赵治宇

***编程模型***

 MapReduce 库的用户用两个函数 ：Map 和 Reduce. 

MapReduce 编程模型的原理是:利用一个输 入key/value pair集合来产生一个输 出的key/value pair集合.

文章中举了一个例子：就是在一个大的文档中计算每个单词出现的的次数，它首先用Map函数输出文档中的每个词、以及这个词的出现次数，Reduce函数把Map函数产生的每一个特定的词的计数累加起来。然后再用户的代码中使用一个可选的调节参数来完成一个符合MapReduce模型规范的对象，在使用中，直接调用MapReduce库，链接在一起，便可实现。

**实现**

 MapReduce模型可以有多种不同的实现方式:有的适用于小型的共享内存方式的机器，有的则适用于大型NUMA架构的多处理器的主机，而有的实现方式更适合人型的网络连接集群。

在文中，作者主要描述了一个适用 Gougle内部广泛使用的运算环境的实现:用以太网交换机连接、由普通PC机组成的大型网络连接集群。

文中作者强调环境包括： 

1. x86 架构、运行Linux操作系统、双处理器、2-4GB内存的机器。

2. 普通的网络硬件设备，每个机器的带宛为百兆或者千兆，但是远小于网络的平均带宽的一半。

3. 集群中包含成百 上千的机器，因此，机器故障是常态。
4. 存储为廉价的内置 IDE硬盘。

  5.用户提交工作(job)给调度系统。

 

原文中作者有一个很清晰的图来说明各个调度之间的关系：![img](https://img2018.cnblogs.com/blog/1532325/201811/1532325-20181127104110033-1429437821.jpg)

 Master持有一些数据结构,它储存每一个Map和Reduce任务的状态，以及Worker机器的标识。

 

 Maser就像一个数据管道，中向文件存储区域的位畳信息通过这个管道从Map传递到Reduce.因此,对于每个已完成的Map任务, master存儲了Map任务产生的文件存儲区域的大小和位置。当Map任务完成时，Maser 接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的Reduce任务。

 

MapReduce中还有着良好的容错机制。因为MapReduce库的设计初衷是使用由成百上千的机器组成的集群来处理超大规模的数据，所以，这个库必须要很好的能够处理机器故障。

***技巧***

 虽然简单的Map和Reduce函数提供的基本功能已经能够满足大部分的计算需要，但是还是发掘出了一些有价值的扩展功能。

文中有以下几个方面 分区函数、顺序保证、Combiner函数、输入和输出类型、副作用、跳过损坏的记录、本地执行、状态信息、计数器方面进行了说明。 

***性能***

这一节则是一个大型集群上运行的两个计算来衡量MapReduce的性能。一个计算在大约1TB的数据中进行特定的模式匹配，另一个计算对大约1TB的数据进行排序。
这两个程序在大量的使用MapReduce的实际应用中是非常典型的一一类是对数据格式进行转换，从一种表现形式转换为另外一种表现形式:另一类是从海量数据中抽取少部分的用户感兴趣的数据。

以下这张图是GREP程序运算随时间的处理过程：

 

 

 ![img](https://img2018.cnblogs.com/blog/1532325/201811/1532325-20181127111204100-1973998391.jpg)

 

 这个图则是

其中Y轴表示输入数据的处理速度。处理速度随着参与MapReduce计算的机器数量的增加而增加，当1764 台worker 参与计算的时，处理速度达到了30GB/s。 当Map任务结束的时候，即在计算开始后80秒，输入的处理速度降到0。整个计算过程从开始到结束一共花了大概150秒。这包括了大约一分钟的初始启动阶段。初始启动阶段消耗的时间包括了是把这个程序传送到各个worker机器上的时间、等待GFS文件系统打开1000个输入文件集合的时间、获取相关的文件本地位置优化信息的时间。

 

以下则是排序过程:

 

![img](https://img2018.cnblogs.com/blog/1532325/201811/1532325-20181127112028169-155392615.jpg)

 

 

显示了这个排序程序的正常执行过程。左上的图显示了输入数据读取的速度。数据读取速度峰值会达到13GB/s，并且所有Map任务完成之后，即大约200秒之后迅速滑落到0。值得注意的是，排序程序输入数据读取速度小于分布式grep程序。这是因为排序程序的Map任务花了大约一半的处理时间和I/O带宽把中间输出结果写到本地硬盘。相应的分布式grep程序的中间结果输出几乎可以忽略不计。

左边中间的图显示了中间数据从Map任务发送到Reduce任务的网络速度。这个过程从第一一个Map任务完成之后就开始缓慢启动了。图示的第-一个高峰是启动了第一批大概1700个Reduce任务，整个MapReduce分布到大概1700台机器上，每台机器1次最多执行1个Reduce任务。排序程序运行大约300秒后，第一-批启动的Reduce任务有些完成了，我们开始执行剩下的Reduce任务。所有的处理在大约600秒后结束。

 

***经验***

 

经验如下：

   \1. 大规模机器学习问题。

   \2. Google News和Froogle产品的集群问题。

   3.从公众查询产品 (比如Google的Zeigeist)的报告中抽取数据。

   4.从大量的新应用 和新产品的网页中提取有用信息( 比如，从大量的位置搜索网页中抽取地理位置信息)。

   5.大规模的图形计算。

***相关***

 MapReduce编程模型在Google内部成功应用于多个领域。原因有这几个方面:首先，由于MapReduce封装了并行处理、容错处理、数据本地化优化、负载均衡等等技术难点的细节，这使得MapReduce库易于使用。即便对于完全没有并行或者分布式系统开发经验的程序员而言，其次，大量不同类型的问题都可以通过MapReduce简单的解决。比如，MapReduce用于生成Google的网络搜索服务所需要的数据、用来排序、用来数据挖掘、用于机器学习，以及很多其它的系统;第三，我们实现了一个在数千台计算机组成的大型集群，上灵活部署运行的MapReduce。这个实现使得有效利用这些丰富的计算资源变得非常简单，因此也适合用来解决Google遇到的其他很多需要大量计算的问题。 

***想法***

简单来说，MapReduce能将海量的数据浓缩成人们想要的数据。Hadoop的两大核心是HDFS和MapReduce，Hadoop的体系结构主要通过HDFS的分布式存储作为底层数据支持的。并且通过MapReduce来进行计算。

 